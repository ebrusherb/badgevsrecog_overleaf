%boring model ~~ null model
% innate case, need to make a ton of assumptions -- at least w learning, all can start at same baseline (which assumptions to make about innate are tricky)

%BOS vs 'status signal' (where info may have more noise)

% what are they paying attention to in IR? describe more 

> BEHECO-2016-0634, Large groups and poor memory favor learning about badges of status over individual recognition
> Brush, Eleanor; Hobson, Elizabeth
>
> Dear Dr. Brush,
>
> I have now received a recommendation from the editor handling your manuscript for Behavioral Ecology, along with reports from two reviewers.  Below please find copies of the comments for authors.
>
> I am afraid that I agree with the referees and the recommendation of the Editor, and I am rejecting your manuscript.  'Behavioral Ecology' receives many more manuscripts than it can publish (currently around two to three times as many), and we have to be severe in our criteria for acceptance.  I am sorry to disappoint you, but I hope you will find the detailed comments of the reviewers helpful.
>
> Best wishes,
> Prof. Leigh Simmons
> Editor-in-Chief, Behavioral Ecology
> leigh.simmons@uwa.edu.au
>
> EDITOR'S COMMENTS
>
> Dear Dr Brush,
>
> Thank you for your submission, which has now been commented on by two reviewers. As you will see, both reviewers welcome the effort to formally model this issue, and are generally positive and encouraging. Having said this, however, both reviewers identify some issues that weaken the paper at present, and which suggest that it is not yet ready for publication. One major issue that both reviewers identify is the treatment of badges of status (BOS) as a learning problem, which, in essence, has the effect of treating individual recognition as an extreme example of BOS. This conflation of BOS with IR means that the model doesn't highlight the crucial difference between the two strategies, and means that the model misses out on important aspects of BOS, such as the fact that they help avoid costly interactions.
> As reviewer 2 notes, it would perhaps be more useful to model specifically the different mechanisms by which BOS are learned. This is where formal modelling would be valuable, and would enable an investigation on whether and how different mechanisms influence BOS and IR evolution.
>
> Having read your paper alongside these comments, I'm afraid I agree with the assessment of our reviewers, and think the paper needs more work before it is ready to be published. Consequently, I'm am sorry to inform you that I will not be recommending your paper for publication. I do hope that you will take our reviewers' comments on board, and develop your model further along the lines they suggest, as it is clear that your work has great potential to shed light on this important issue.
>
> Once again, I'm sorry to be the bearer of bad news, but we receive many more papers than we have space to publish, and we have to make some very tough decisions at times.
>
> With best wishes,
>
> Louise
>
>
> Prof. Louise Barrett
> Editor, Behavioral Ecology
>
> REVIEWERS' COMMENTS
>
> Reviewer: 1
>
> Comments to the Author
> I will just go ahead and reveal my identity (Thore Bergman) because it will become obvious soon enough! This paper is very exciting and I think it is important to use models to test ideas about assessment strategies. This type of work is exactly what we hoped for in our previous paper (Sheehan and Bergman, 2016). Thus, I think this paper represents an important advance. I do, however, see some fairly significant areas for improvement. I have three main concerns.
>
> First, (this is certainly my ego-centric view so please feel free to dismiss it, but) I see the current manuscript as an important follow-up and test of some of the ideas we proposed in Sheehan and Bergman 2016 (note that we first described the group size relationship to assessment strategies in Bergman and Sheehan 2013).  I think the introduction could be changed a bit to describe our work and use that as more of a starting point/motivating factor. The papers are largely congruent with, for example, striking similarity between S & B 2016 Figure 1C and current Figure 2A and (to a lesser extent) between S & B 2016 Figure 1D and current Figure 5A. The current manuscript is an important advancement because it adds the rigor of a model to what was previously a theoretical argument. It also explores memory duration as an important variable. However, the ideas and predictions are in the literature.
%this was an interesting response -- I thought we did a good job of attributing to past work. Probably will need to make this (even more) clear
>
> Second, I don’t think the model and manuscript characterize the use of badges of status very well. In general, I don’t see the use of badges of status as a ‘learning’ problem. Yes, in some cases animals need to learn about which version of the badge is the ‘good’ one but that is not always the case. Animals can have an innate preference for (or stronger reaction to) bigger, brighter, showier versions of the badge. More importantly, even in cases where learning is involved, the learning can occur at a completely different life phase from when the assessment occurs. See, for example, work by ten Cate and others on peak shift phenomena in dimorphic birds. Animals have an entire juvenile period to learn about signal/quality relationships so it is unlikely that further learning is needed by the time they are actually choosing among rivals or mates. My main worry is that calling this a learning situation fails to highlight the critical difference between the assessment strategies; recognition requires at least one previous interaction with the individual being assessed while signals do not. Furthermore, I do not see why the model requires animals using signals to assess all other members of the group and to do so categorically. The only time animals would need to assess everyone else is if they were searching for the ‘best’ individual in the group, but that seems like an unusual case. More typically, animals are making a yes/no decision about one individual that they can currently observe (fight or not, mate or not). In such cases, classification that is above or below a certain threshold (2 categories) is sufficient and they don't need to remember what category an animal was in. Thus, they may not often care about the relative classification of third party individuals. If they do have to choose among multiple individuals simultaneously, a simple relative comparison at the time of the assessment (that does not require memory of categorical information) would likely suffice. Is there any evidence that animals actually use multiple categories to rank others based on their signals? It is also only in this (seemingly unnatural) description of how signals are used that individual recognition is the extreme case of signal use. Finally, the model seems to miss an important aspect of badges of status. One of the main benefits of a signaling system is that it allows individuals to avoid costly interactions. Thus, the presence of a signal should make interactions less likely, particularly when quality is very disparate. Having said all that, I think the model is still worthwhile, but some care may need to be taken in presenting it as an exploration of what may be a special case of how animals use badges of status.
% also interesting, and may be an easy area to expand. I see the learning of badges vs learning about individuals as the most interesting cases. But I guess we can throw in some of the more boring stuff...?

%add memory comparisons, short BOS window vs long IR, etc.
>
> Third, it would be great to see some exploration of how the two modes of assessment interact with each other. Does the presence of a signal reduce the utility of individual recognition (or the reverse) as we proposed? This may be beyond the scope of this manuscript but it something to consider in the future.
% yeah, probably beyond the scope of what we want to do first. But good to keep in mind for the next bunch of papers!
>
> Here are some more minor points:
>
> Line 14: I think this would be a good place to point out that others have suggested that badges of status and recognition are likely to be useful in different sized groups.  
>
> Line 35-36: This needs citations. I don’t think the use of signals necessarily implies a categorical assessment (see comments above).
>
> Line 38-39: “Assessment’ could be introduced here (or even earlier).
>
> Line 43. It is fine to cite Sheehan and Bergman 2016 as saying this but it would be useful to also say that we did compare the two systems directly.
>
> Line 90: In the absence of a badge of status, it is not clear how a dominance hierarchy could form without individual recognition? The recognition must come first. That said, the references are cited accurately.
>
> Line 97: This is the number of people that humans maintain stable relationships with. We can recognize many more faces than this.
>
> Line 101-103: I disagree. I think they are fundamentally different solutions. One requires experience with the animal being assessed, while the other does not.
>
> Line 140: we did not predict that the stability of the badge mattered—it is the stability of the underlying feature that is being signaled that matters. For example, we suggest that this is why you often have fertility signals in primates that otherwise rely heavily on recognition. The information from previous encounters is unreliable for such a dynamic trait.
>
> Line 321-329: I would say we focused on systems where no further learning was required at the time of assessment (either because it was innate or because the learning had already occurred).
>
> Line 374: remove “require”
>
> Line 375-376: This is a very interesting idea. Are there examples of eavesdropping in badge of status signaling systems?
>
> Line 401: Make it clear that you are not the first to generate some of the predictions.
>
>
> Reviewer: 2
>
% intermediate model where individuals learn slope of relationship between quality & badge -- use slope to make inferences about new 
> Comments to the Author
> Review of ‘Large groups and poor memory favor learning about badges of status of individual recognition’
>
> The paper presents a formal model based on previously described verbal models examining the relationship between badges of status and individual recognition. The topic is very timely and desperately in need for formal treatment, so I applaud the authors for that. While the model they present shows some promise, there are a number of issues I think are critical for them to address prior to publication. Specifically, the model seems to assume equivalence between the learning that would occur with individual recognition and with badges of status, though there are theoretical and empirical reasons to suggest that is inappropriate. Additionally, the model does not involve any optimization or dynamic evolution and so is a static rendering of the verbal models on which it is based.
>
> Major comments
> (1) Learning in individual recognition (IR) and badge of status (BOS) systems
>
> While it is clear that IR of group members must utilize flexible learning and memory, the proposed mechanism of BOS learning put forth here strikes me as problematic given that little is known about how exactly BOS signals come to be interpreted in most species and there is no a priori reason to expect learning. Badges of status tend to be single trait that vary along a single to a few continuous dimensions (e.g. bib size in sparrows). There is no need that badges be learned. It is completely plausible that the rules of how badges correlate with quality are known innately (whether this is the case in any given species is a subject for empirical study). Even if badges are learned, there is a single rule to be learned – e.g. larger color patch = more dominant. From what I can understand of what is described here, badges are learned in categories, but there is no ordering to them in the mind of the agents involved.
>
> The authors cite the idea from Barnard and Burk that fine-grained learning about BOS is equivalent to IR. I would strongly challenge this antiquated notion. It has been clear since that early work that signals involved in signaling identity (i.e. that are used for IR) are fundamentally different than those that signal quality (i.e. that are a BOS). Dale (2006) in Bird Coloration provides a nice overview of signal design that is relevant to this topic. Notably, IR is only really feasible when there are multiple variable traits. There are some recent publications as well that examine the evolution of identity signals and find evidence for selection favoring combinatorial variation in those traits, backing up Dale’s predictions. While it is in theory possible to learn individual characteristics from BOS, that is very hard. Consider a species that varies continuously in color as the supplemental example from the paper suggests. While it is easy to align everyone along a continuum and to differentiate between very different color, distinguishing similar individuals is a challenge. It would be like trying to recognize individuals based on their height alone…it would be very challenging. If multiple different traits vary independently, however, the combinatorial variation that create provides a much easier problem for animals to solve
>
> The point I am trying to make here is that even though it is possible to measure individual differences in badges, BOS is not just a crude categorization version of IR – they are two fundamentally different processes both in terms of signal design and signal perception. The present paper appears to treat IR as just an extreme version of BOS categories and that is wrong.
>
> On this topic, given the unknown nature of how BOS systems are learned for most species, it would be enlightening for a revised version of this manuscript to consider different mechanisms of learning BOS – (1) innate understanding, (2) learning of a rule in addition to  (3) learning and updating of different categories that is presently in the paper. Given that different strategies for understanding BOS may exist, the effect that has, if any, on BOS/IR evolution would be a useful contribution.
>
> Even given the current modeling of BOS learning, applying the same learning model to the two systems seems unrealistic; most likely different neural mechanism for learning and storing stable BOS information versus individual recognition. For IR, it would make sense to store information about individual on first encounter in short-term bank, then if frequency of encounters reach a certain threshold store this information in medium or long-term memory (with a different corresponding value of w). The overall cognitive limitation for individual recognition might then be the number of individuals one can remember long-term.  For BOS, once a badge is deemed stable (reaches a threshold frequency or number of encounters) the information about the badge should be stored in long-term memory with a longer memory window w.
>
>
> (2) The model is a static rendering of a verbal model
>
> The value of the model would be greatly improved if the parameters used were optimized rather than arbitrarily fixed. Another useful analysis would be to examine under what conditions IR or BOS can invade a population using the other method. I don’t think both of these would be necessary in a revised manuscript, but I think something more dynamic would greatly improve the manuscript.
% didn't we do some parameter space exploration? Or am I not understanding this comment?
>
> Additional comments:
>
> All individuals are experienced/knowledgeable enough about the population to have established a stable baseline of expectations tuned to the true distribution of quality in the population. (bi(t) is invariant, does not evolve as individual learns more about population)
>
> The speed of learning/ updating assessment (parameter ) does not vary with respect to the divergence from expectation or strength of accumulated evidence building that expectation, nor is it optimally tuned to minimize the cost by balancing the cost of error, learning time and cognitive investment.
>
> The threshold of acceptable error in assessment used to determine learning time is fixed at 0.2, rather than optimally tuned to minimize the overall cost by balancing cost of error and cost of time spent learning.
>
> Cost terms:
>
> Parameters  and threshold error (currently fixed at 0.2) should be optimally tuned so as to minimize the overall cost. % this would be interesting
>
> If individuals are assumed to interact/observe for τ time units, then the relative assessment error term used to determine cost should be the threshold error, not the average error after T time units. This threshold error should be tuned based on how costly errors and spending time learning are.
>
> Results:
>
> Lines 268-288: Not informative to discuss results that arise from arbitrarily generated parameters. Describing how parameter changes model results does not translate to real world unless there is also discussion of how parameter relates to real world drivers.
>
> Line 335: The comment that we might expect it would always be advantageous to improve cognitive abilities ignores the cost tradeoff (C increases as δ decreases).
>
> Lines 332-342: The comparison to bias-variance tradeoff is problematic. The bias-variance tradeoff in classification problems describes the costs/benefits of overfitting or underfitting a set of models to their training sets. The more tightly each model adheres to a training set, the less bias there will be (model points lie closer to data points), but the more variance there will be between different realizations of the model constructed from different training sets; in other words the models will adhere well to their specific given training sets but be less generalizable.  In this case, accuracy is compromised when δ is too low because of a disruption in the continuous learning process due to lower probability of repeat encounter. Specificity in this case is not punished due to lack of generalizability and subsequent mis-classification (as in the bias-variance tradeoff) but rather due to its influence in limiting repeat occurrences, which are needed for overall long-run accuracy.
>
> Line 368: Is there an inherent deficit in the categorical method? And if so, why should we expect more observation to help? The categorical method evolves in circumstances where generalizing is a favorable strategy because there are too many individuals/ too short of a memory window to ensure continuous learning (required for long-run accuracy) when δ = 0; it does not seem correct to consider this strategy a “deficit”. The categorical method reduces reliance on any one encounter, since repeat encounters within the same category are sufficient to allow for continuous learning.  The value of observational learning is not so much in its actual contribution to the assessment update (since it is a slower and more noisy type of learning), but rather in ensuring the continuation of learning. In the BOS model, continued learning is already more likely because the probability of encountering a given category is higher than that of encountering a given individual. So it is intuitive that reliance on observation in BOS is lower than in IR, given the same N.
>
> Line 370: “several species capable of both observational learning and IR…” but without comment on the converse this does not do much to bolster the argument. Are species that use BOS incapable of observational learning? An open question perhaps but still worth comment.
>
> Line 386: benefits do not outweigh cost of cognitive ability; this is an artificial result arising from the specific tradeoff parameters selected in the model. Again in Line 392-399: limit comments on results to general patterns/relationships rather than absolute terms.
>
> Conclusions
> The basic finding that IR is better for small groups is not novel, but does confirm previous verbal models (Rohwer 1982, Sheehan & Bergman 2016). There are additional predictions that come out of this model and I would recommend that the authors highlight which features of there model confirm previous work and which provide novel insights. As it is now, the novelty of some of the findings gets lost in the weeds.
